{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Entropy\n",
    "\n",
    "We are in a position to discuss information entropy.  This will give us a\n",
    "powerful perspective on how information passes between experiments, and will\n",
    "prove important in certain machine learning algorithms.\n",
    "\n",
    "There used to be a TV game show where the host would hide a prize behind\n",
    "one of three doors and the contestant would have to pick one of the doors.\n",
    "However, before opening the door of the contestant's choice, the host\n",
    "would open one of the other doors and ask the contestant if she wanted to\n",
    "change her selection.  This is the classic *Monty Hall* problem. The\n",
    "question is should the contestant stay with her original choice or switch\n",
    "after seeing what the host has revealed?  From the information theory\n",
    "perspective, does the information environment change when the host reveals\n",
    "what is behind one of the doors?  The important detail here is that the\n",
    "host *never* opens the door with the prize behind it, regardless of the\n",
    "contestant's choice. That is, the host *knows* where the prize is, but he\n",
    "does not reveal that information directly to the contestant. This is the\n",
    "fundamental problem information theory addresses --- how to aggregate and\n",
    "reason about partial information. We need a concept of information that\n",
    "can accommodate this kind of question.\n",
    "\n",
    "## Information Theory Concepts\n",
    "\n",
    "The Shannon *information content* of an outcome $x$ is defined as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x) = \\log_2\\frac{1}{P(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $P(x)$ is the probability of $x$.  The *entropy* of the ensemble\n",
    "$X$ is defined to be the Shannon information content of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X) = \\sum_x P(x) \\log_2 \\frac{1}{P(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is no accident that the entropy has this functional form\n",
    "as the expectation of $h(x)$. It leads to a deep and powerful theory\n",
    "of information. \n",
    "\n",
    "To get some intuition about what information entropy means, consider a sequence\n",
    "of three-bit numbers where each individual bit is equally likely. Thus, the\n",
    "individual information content of a single bit is $h(x) = \\log_2 (2) = 1$. The\n",
    "units of entropy are *bits* so this says that information content of a single bit\n",
    "is one bit. Because the three-bit number has elements that are mutually\n",
    "independent and equally likely, the information entropy of the\n",
    "three-bit number is $h(X) = 2^3 \\times  \\log_2(2^3)/8=3 $. Thus,\n",
    "the basic idea of information content at least makes sense at this level.\n",
    "\n",
    "A better way to interpret this question is as how much information would I have\n",
    "to provide in order to uniquely encode an arbitrary three-bit number? In this\n",
    "case, you would have to answer three questions: *Is the first bit zero or one?\n",
    "Is the second bit zero or one? Is the third bit zero or one?* Answering\n",
    "these questions uniquely specifies the unknown three-bit number. Because the\n",
    "bits are mutually independent, knowing the state of any of the bits does not\n",
    "inform the remainder.\n",
    "\n",
    "Next, let's consider a situation that lacks this mutual independence. Suppose\n",
    "in a group of nine otherwise identical balls there is a heavier  one.\n",
    "Furthermore, we also have a measuring scale that indicates whether one side is\n",
    "heavier, lighter, or equal to the other.  How could we identify the heavier\n",
    "ball? At the outset, the information content, which measures the uncertainty of\n",
    "the situation is $\\log_2(9)$ because one of the nine balls is heavier. [Figure](#fig:Information_Entropy_001) shows\n",
    "one strategy. We could arbitrarily select out one of the balls (shown by the\n",
    "square), leaving the remaining eight to be balanced. The thick, black\n",
    "horizontal line indicates the scale.  The items below and above this line\n",
    "indicate the counterbalanced sides of the scale.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/NineBall_01.jpg, width=500 frac=0.75] One heavy ball is hidden among eight identical balls. By weighing groups sequentially, we can determine the heavy ball. <div id=\"fig:Information_Entropy_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Information_Entropy_001\"></div>\n",
    "\n",
    "<p>One heavy ball is hidden among eight identical balls. By weighing groups sequentially, we can determine the heavy ball.</p>\n",
    "<img src=\"fig-probability/NineBall_01.jpg\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "If we get lucky, the scale will report that the group of four walls on either\n",
    "side of the balance are equal in weight. This means that the ball that was\n",
    "omitted is the heavier one. This is indicated by the hashed left-pointing\n",
    "arrow. In this case, all the uncertainty has evaporated, and the *informational\n",
    "value* of that one weighing is equal to $\\log_2(9)$.  In other words, the scale\n",
    "has reduced the uncertainty to zero (i.e., found the heavy ball). On the other\n",
    "hand, the scale could report that the upper group of four balls is heavier\n",
    "(black, upward-pointing arrow) or lighter (gray, downward-pointing arrow). In\n",
    "this case, we cannot isolate the heavier ball until we perform all of the\n",
    "indicated weighings, moving from left-to-right. Specifically, the four balls on\n",
    "the heavier side have to be split by a subsequent weighing into two balls and\n",
    "then to one ball before the heavy ball can be identified.  Thus, this process\n",
    "takes three weighings. The first one has information content $\\log_2(9/8)$, the\n",
    "next has $\\log_2(4)$, and the final one has $\\log_2(2)$.  Adding all these up\n",
    "sums to $\\log_2(9)$. Thus, whether or not the heavier ball is isolated in the\n",
    "first weighing, the strategy consumes $\\log_2(9)$ bits, as it must, to find the\n",
    "heavy ball.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/NineBall_02.jpg, width=500 frac=0.75] For this strategy, the balls are broken up into three groups of equal size and subsequently weighed.  <div id=\"fig:Information_Entropy_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Information_Entropy_002\"></div>\n",
    "\n",
    "<p>For this strategy, the balls are broken up into three groups of equal size and subsequently weighed.</p>\n",
    "<img src=\"fig-probability/NineBall_02.jpg\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "However, this is not the only strategy. [Figure](#fig:Information_Entropy_002)\n",
    "shows another.  In this approach, the nine balls are split up into three groups\n",
    "of three balls apiece. Two groups are weighed. If they are of equal weight,\n",
    "then this means the heavier ball is in the group that was left out (dashed\n",
    "arrow). Then, this group is split into two groups, with one element left out.\n",
    "If the two balls on the scale weigh the same, then it means the excluded one is\n",
    "the heavy one. Otherwise, it is one of the balls on the scale. The same process\n",
    "follows if one of the initially weighed groups is heavier (black upward-facing\n",
    "arrow) or lighter (gray lower-facing arrow). As before the information content\n",
    "of the situation is $\\log_2(9)$. The first weighing reduces the uncertainty of\n",
    "the situation by $\\log_2(3)$ and the subsequent weighing reduces it by another\n",
    "$\\log_2(3)$.  As before, these sum to $\\log_2(9)$, but here we only need two\n",
    "weighings whereas the first strategy in [Figure](#fig:Information_Entropy_001) takes\n",
    "an average of $1/9 + 3*8/9 \\approx 2.78$ weighings, which is more than two\n",
    "from the second strategy in [Figure](#fig:Information_Entropy_002).\n",
    "\n",
    "Why does the second strategy use fewer weighings?  To reduce weighings, we need\n",
    "each weighing to adjudicate equally probable situations as many times as\n",
    "possible. Choosing one of the nine balls at the outset (i.e, first strategy in\n",
    "[Figure](#fig:Information_Entropy_001)) does not do this because the\n",
    "probability of selecting the correct ball is $1/9$. This does not create a\n",
    "equiprobable situation in the process. The second strategy leaves an equally\n",
    "probable situation at every stage (see [Figure](#fig:Information_Entropy_002)), so it extracts the most information out of\n",
    "each weighing as possible. Thus, the information content tells us how many bits\n",
    "of information have to be resolved using *any* strategy (i.e., $\\log_2(9)$ in\n",
    "this example). It also illuminates how to efficiently remove uncertainty;\n",
    "namely, by adjudicating equiprobable situations as many times as possible.  \n",
    "\n",
    "## Properties of Information Entropy\n",
    "\n",
    "Now that we have the flavor of the concepts, consider the following properties\n",
    "of the information entropy,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X) \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with equality if and only if $P(x)=1$ for  exactly one $x$.\n",
    "Intuitively, this means that when just one of the items in the ensemble is\n",
    "known absolutely (i.e., with $P(x)=1$), the uncertainty collapses to zero.\n",
    "Also note that entropy is maximized when $P$ is uniformly distributed across\n",
    "the elements of the ensemble. This is illustrated in [Figure](#fig:Information_Entropy_003) for the case of two outcomes. In other words,\n",
    "information entropy is maximized when the two conflicting alternatives are\n",
    "equally probable. This is the mathematical reason why using the scale in the\n",
    "last example to adjudicate equally probable situations was so useful for\n",
    "abbreviating the weighing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots\n",
    "import numpy as np\n",
    "p = np.linspace(0,1,50)\n",
    "fig,ax=subplots()\n",
    "#fig.set_size_inches((14,7))\n",
    "_=ax.plot(p,p*np.log2(1/p)+(1-p)*np.log2(1/(1-p)),'k-')\n",
    "_=ax.set_xlabel('$p$',fontsize=24)\n",
    "_=ax.set_ylabel('$H(p)$',fontsize=24)\n",
    "_=ax.grid()\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/information_entropy_003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/information_entropy_003.png, width=500 frac=0.75] The information entropy is maximized when $p=1/2$.  <div id=\"fig:Information_Entropy_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Information_Entropy_003\"></div>\n",
    "\n",
    "<p>The information entropy is maximized when $p=1/2$.</p>\n",
    "<img src=\"fig-probability/information_entropy_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Most importantly, the concept of entropy extends jointly as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X,Y) = \\sum_{x,y} P(x,y) \\log_2 \\frac{1}{P(x,y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If and only if $X$ and $Y$ are independent, entropy becomes\n",
    "additive,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X,Y) =  H(X)+H(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence\n",
    "\n",
    "Notions of information entropy lead to notions of distance between probability\n",
    "distributions that will become important for machine learning methods.  The\n",
    "Kullback-Leibler divergence between two probability distributions $P$ and $Q$\n",
    "that are defined over the same set is defined as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{KL}(P,Q) = \\sum_x P(x) \\log_2 \\frac{P(x)}{Q(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that $D_{KL}(P,Q) \\ge 0$ with equality if and only if $P=Q$.\n",
    "Sometimes the Kullback-Leibler divergence is called the Kullback-Leibler\n",
    "distance, but it is not formally a distance metric because it is asymmetrical\n",
    "in $P$ and $Q$. The Kullback-Leibler divergence defines a relative entropy as\n",
    "the loss of information if $P$ is modeled in terms of $Q$.  There is an\n",
    "intuitive way to interpret the Kullback-Leibler divergence and understand its\n",
    "lack of symmetry.  Suppose we have a set of messages to transmit, each with a\n",
    "corresponding probability $\\lbrace\n",
    "(x_1,P(x_1)),(x_2,P(x_2)),\\ldots,(x_n,P(x_n)) \\rbrace$. Based on what we know\n",
    "about information entropy, it makes sense to encode the length of the message\n",
    "by $\\log_2 \\frac{1}{p(x)}$ bits. This parsimonious strategy means that more\n",
    "frequent messages are encoded with fewer bits. Thus, we can rewrite the entropy\n",
    "of the situation as before,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X) = \\sum_{k} P(x_k) \\log_2 \\frac{1}{P(x_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, suppose we want to transmit the same set of messages, but with a\n",
    "different set of probability weights,  $\\lbrace\n",
    "(x_1,Q(x_1)),(x_2,Q(x_2)),\\ldots,(x_n,Q(x_n)) \\rbrace$.  In this situation, we\n",
    "can define the cross-entropy as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_q(X) = \\sum_{k} P(x_k) \\log_2 \\frac{1}{Q(x_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that only the purported length of the encoded message has\n",
    "changed, not the probability of that message. The difference between these two\n",
    "is the Kullback-Leibler divergence,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{KL}(P,Q)=H_q(X)-H(X)=\\sum_x P(x) \\log_2 \\frac{P(x)}{Q(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this light, the Kullback-Leibler divergence is the average\n",
    "difference in the  encoded lengths of the same set of messages under two\n",
    "different probability regimes. This should help explain the lack of symmetry of\n",
    "the Kullback-Leibler divergence --- left to themselves, $P$ and $Q$ would\n",
    "provide the optimal-length encodings separately, but there can be no necessary\n",
    "symmetry in how each regime would rate the informational value of each message\n",
    "($Q(x_i)$ versus $P(x_i)$). Given that each encoding is optimal-length in its\n",
    "own regime means that it must therefore be at least sub-optimal in another,\n",
    "thus giving rise to the Kullback-Leibler divergence. In the case where the\n",
    "encoding length of all messages remains the same for the two regimes, then the\n",
    "Kullback-Leibler divergence is zero [^Mackay].\n",
    "\n",
    "[^Mackay]: The best, easy-to-understand presentation of this material is chapter\n",
    "four of Mackay's text [[mackay2003information]](#mackay2003information). Another good reference is\n",
    "chapter four of [[hastie2013elements]](#hastie2013elements).\n",
    "\n",
    "\n",
    "## Cross-Entropy as Maximum Likelihood\n",
    "\n",
    "<!-- # See Goodfellow, p.147. -->\n",
    "\n",
    "Reconsidering maximum likelihood from our statistics chapter in more \n",
    "general terms, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{\\textnormal{ML}} =\\argmax_{\\theta} \\sum_{i=1}^n \\log p_{\\textnormal{model}}(x_i;\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $p_{\\textnormal{model}}$ is the assumed underlying probability\n",
    "density function parameterized by $\\theta$ for the $x_i$ data elements.\n",
    "Dividing the above summation by $n$ does not change the derived optimal values,\n",
    "but it allows us to rewrite this using the  empirical density function for $x$\n",
    "as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{\\textnormal{ML}} = \\argmax_{\\theta} \\mathbb{E}_{x\\sim \\hat{p}_{\\textnormal{data}}}(\\log p_{\\textnormal{model}}(x_i;\\theta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that we have the distinction between $p_{\\textnormal{data}}$ and\n",
    "$\\hat{p}_{\\textnormal{data}}$ where the former is the unknown distribution of\n",
    "the data  and the latter is the estimated distribution of the data we have on\n",
    "hand.\n",
    "\n",
    "The cross-entropy can be written as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{KL}(P,Q) = \\mathbb{E}_{X\\sim P}(\\log P(x))-\\mathbb{E}_{X\\sim P}(\\log Q(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $X\\sim P$ means the random variable $X$ has distribution $P$.\n",
    "Thus, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{\\textnormal{ML}} = \\argmax_{\\theta} D_{KL}(\\hat{p}_{\\textnormal{data}}, p_{\\textnormal{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is, we can interpret maximum likelihood as the cross-entropy\n",
    "between the $p_{\\textnormal{model}}$ and the $\\hat{p}_{\\textnormal{data}}$\n",
    "distributions. The first term has nothing to do with the estimated $\\theta$ so \n",
    "maximizing this is the same as minimizing the  following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_{x\\sim \\hat{p}_{\\textnormal{data}}}(\\log p_{\\textnormal{model}}(x_i;\\theta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because information entropy is always non-negative. The important\n",
    "interpretation is that maximum likelihood is an attempt to choose $\\theta$\n",
    "model parameters that make the empirical distribution of the data match the\n",
    "model distribution."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
